# -*- coding: utf-8 -*-
"""kakollu_sokoban

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pZpwx4yCABqUL90VeG_8AXF7Ga5m9yC-
"""

import gym
import numpy as np
from matplotlib import pyplot
class sokoban(gym.Env):
    def __init__(self):
        # self.s = [
        #     [8, 8, 8, 8, 8, 8],
        #     [8, 0, 0, 0, 0, 8],
        #     [8, 9, 4, 0, 0, 8],
        #     [8, 0, 0, 0, 6, 8],
        #     [8, 0, 0, 0, 0, 8],
        #     [8, 8, 8, 8, 8, 8],
        # ]
        self.s = [[8, 8, 8, 8, 8],       # Initial State
                  [8, 0, 0, 0, 8],
                  [8, 9, 4, 0, 8],
                  [8, 0, 6, 0, 8],
                  [8, 8, 8, 8, 8],]
        # defining state
        # 9 - agent
        # 4 -  box
        # 6 -goal state
        # 8 - wall
        self.a = [3, 2, 1, 4]     # Defining action space
        # defining action values
        # 1 - up
        # 2 -right
        # 3 - left
        # 4 - down
        self.reward = 0
        self.done = False

    def reset(self):       # defining reset function which returns initial state when it called
       
        self.s = [[8, 8, 8, 8, 8],
                  [8, 0, 0, 0, 8],
                  [8, 9, 4, 0, 8],
                  [8, 0, 0, 6, 8],
                  [8, 8, 8, 8, 8],]
        return self.s
       

    def step(self, a):     # defining step function to perform actions taken from epsilon greedy function
        state = self.s
        S = state
    

        action = a
        if action == 1:
            next_state, reward, done = self.UP(S)       # Calling functions based on action selected
            return next_state, reward, done             # returning next state, reward and status of the game 
        if action == 2:          
            next_state, reward, done = self.RIGHT(S)
            return next_state, reward, done
        if action == 3:
            next_state, reward, done = self.LEFT(S)
            return next_state, reward, done
        if action == 4:
            next_state, reward, done = self.DOWN(S)
            return next_state, reward, done

    def DOWN(self, S):                                  # Defining function for action UP
        for i in range(len(S)):
            for j in range(len(S)):
              # find agent and find empty tile
                if S[i][j] == 9 and S[i + 1][j] == 0:     # Checking if the next tile of a player is empty
                    S[i + 1][j] = 9
                    S[i][j] = 0                             # Updating player location if the above condition is true
                    reward = 1
                    self.done = False
                    break
                
                # find agent and wall
                if S[i][j] == 9 and S[i + 1][j] == 8:         # checking if the adjacent tile of a player is a wall
                    reward = -3                               # If the condition is true no action will perform and the status will be false
                    self.done = False
                    break
                # find agent and box and next empty tile
                if S[i][j] == 9 and S[i + 1][j] == 4 and S[i + 2][j] == 0:        # checking if the adjacent tile of a box is empty
                    S[i + 1][j] = 9                                                # If the condition satisfies it will update the position of a box and a player
                    S[i + 2][j] = 4
                    S[i][j] = 0
                    reward = 2
                    self.done = False
                    break
                if S[i][j] == 9 and S[i + 1][j] == 4 and S[i + 2][j] == 8:         # checking if the adjacent tile of a box is a wall and player tries to push
                    reward = -10                                                    # if the condition satisfies then no action will perfrom and it will stop the game by changing the status to True
                    self.done = True
                    break
                # reaching target
                if S[i][j] == 9 and S[i + 1][j] == 4 and S[i + 2][j] == 6:        # checking if the adjacent tile of a box is a goal state and the player tries to push
                    reward = 100                                                    # if this condition satisfies then the game will complete by changing status to true
                    self.done = True
                    break
                if S[i][j] == 9 and S[i + 1][j] == 6:                             # checking if there is a goal state adjacent to player
                    reward = 1                                                    # if the condition satisfies then no action will perform and it returns the previous state
                    self.done = False
                    break
                if S[i][j] == 4:                                                  # if the box is in corner for action perform then it will stop the game and change status to true 
                    if (i == 1 and j == 1) or (i == 1 and j == len(S) - 2) or (i == len(S) -2 and j == 1) or (i == len(S) - 2 and j == len(S) - 2):
                        self.done = True
                        reward = -10
                        break
        return S, reward, self.done

    def UP(self, S):
        for i in range(len(S)):
            for j in range(len(S)):
                if S[i][j] == 9 and S[i - 1][j] == 0:
                    S[i - 1][j] = 9
                    S[i][j] = 0
                    reward = 1
                    self.done = False
                    break
                if S[i][j] == 9 and S[i - 1][j] == 8:
                    reward = -3
                    self.done = False
                    break
                if S[i][j] == 9 and S[i - 1][j] == 4 and S[i - 2][j] == 0:
                    S[i - 1][j] = 9
                    S[i - 2][j] = 4
                    S[i][j] = 0
                    reward = 2
                    self.done = False
                    break
                if S[i][j] == 9 and S[i - 1][j] == 4 and S[i - 2][j] == 8:
                    reward = -10
                    self.done = True
                    break
                if S[i][j] == 9 and S[i - 1][j] == 4 and S[i - 2][j] == 6:
                    reward = 100
                    self.done = True
                    break
                if S[i][j] == 9 and S[i - 1][j] == 6:  
                    reward = 1
                    self.done = False
                    break
                if S[i][j]==4:
                   if (i == 1 and j == 1) or (i == 1 and j == len(S) - 2) or (i == len(S) -2 and j == 1) or (i == len(S) - 2 and j == len(S) - 2):
                         self.done = True
                         reward = -10
                         break
        return S, reward, self.done

    def RIGHT(self, S):                                         # defining right function to check all the conditions when it performs and returns new state, reward, and status of the game
      for i in range(len(S)):                                   
        for j in range(len(S)):
          if S[i][j] == 9 and S[i][j + 1] == 0:
              S[i][j + 1] = 9
              S[i][j] = 0
              reward = 1
              self.done = False
              break
          if S[i][j] == 9 and S[i][j + 1] == 8:
              reward = -3
              self.done = False
              break
          if S[i][j] == 9 and S[i][j + 1] == 4 and S[i][j + 2] == 0:
              S[i][j + 1] = 9
              S[i][j + 2] = 4
              S[i][j] = 0
              reward = 2
              self.done = False
              break
          if S[i][j] == 9 and S[i][j + 1] == 4 and S[i][j + 2] == 8:
              reward = -10
              self.done = True
              break
          if S[i][j] == 9 and S[i][j + 1] == 4 and S[i][j + 2] == 6:
              reward = 100
              self.done = True
              break
          if S[i][j] == 9 and S[i][j + 1] == 6: 
              reward = 1
              self.done = False
              break
          if S[i][j]==4:
            if (i == 1 and j == 1) or (i == 1 and j == len(S) - 2) or (i == len(S) -2 and j == 1) or (i == len(S) - 2 and j == len(S) - 2):
                  self.done = True
                  reward = -10
                  break

      return S, reward, self.done

    def LEFT(self, S):                                    # Defining left function to check all the conditions when the action performs and returns new state, reward, and status of the game
      for i in range(len(S)):
        for j in range(len(S)):
          if S[i][j] == 9 and S[i][j - 1] == 0:
              S[i][j - 1] = 9
              S[i][j] = 0
              reward = 1
              self.done = False
              break
          if S[i][j] == 9 and S[i][j - 1] == 8:
              reward = -3
              self.done = False
              break
          if S[i][j] == 9 and S[i][j - 1] == 4 and S[i][j - 2] == 0:
              S[i][j - 1] = 9
              S[i][j - 2] = 4
              S[i][j] = 0
              reward = 2
              self.done = False
              break
          if S[i][j] == 9 and S[i][j - 1] == 4 and S[i][j - 2] == 8:
              reward = -10
              self.done = True
              break
          if S[i][j] == 9 and S[i][j - 1] == 4 and S[i][j - 2] == 6:
              reward = 100
              self.done = True              
              break
          if S[i][j] == 9 and S[i][j - 1] == 6:  
              
              reward = 1
              self.done = False
              break
          if S[i][j]==4:
            if (i == 1 and j == 1) or (i == 1 and j == len(S) - 2) or (i == len(S) -2 and j == 1) or (i == len(S) - 2 and j == len(S) - 2):
                self.done = True
                reward = -10

                break
      return S, reward, self.done


sokoban_1 = sokoban()
env = sokoban()

def sarsa(env, alpha, eps,E):         # defining semi-gradient sarsa algorithm and passing alpha, epsilon and episode values
    w = np.zeros(5) 
    # w = [-10,-10,-10,-10,-10]                # Intializing weight vector with zeros
    # E = 100
    y = []
    z = []
    for i in range(E):
        s = env.reset()             # reset the state for each episode
        a = e_greedy(env, s, w, eps)      # Choosing action using epsilon greedy function
        print("EPISODE ---------------------------> ", i + 1)
        game_status = False
        while not game_status:                 # Iterating until the game completes
            s_p, r, done = env.step(a)        # passing the chosen action to a step function for a next state, reward and status
            if done == True:                  # if the status of a new state is true then it will update the weights using previous state and action
                w = w + alpha * (r - q_hat(s, a, w)) * feature_vector(s, a)
                game_status = True
                s = env.reset()               # The game will be reset for every time it completes 
                env.done = False
            else:
                a_p = e_greedy(env, s_p, w, eps)            #if the status of the game is false then it will choose next action by using new state
                w = w + alpha * (
                    r + 0.9 * q_hat(s_p, a_p, w) - q_hat(s, a, w)         # weights will update by using updated state and updated chosen action
                ) * feature_vector(s, a)
                a = a_p
                s = s_p
        z.append(i)
    
        x = []
        for i in range(len(env.a)):
          x.append(q_hat(env.s,i,w))
        y.append(np.max(x))  
    pyplot.plot(z, y)
    pyplot.show
    return 

def e_greedy(env, s, w, eps):     # defining epsilon greedy function
    random_int = np.random.random()         # if the random number is greater than epsilon value then it will exploit
    if random_int > eps:
        max_a = 0                           # initialize maximum action and maximum action value
        max_a_v = -np.Inf
        for a in env.a:             # choosing actions from environment action space
            v = q_hat(s, a, w)
            if v > max_a_v:      # if the q_hat value is greater than previous maximum action value then it will choose that action as maximum action
                max_a = a
                max_a_v = v
        return max_a
    else:                         # if the random number is less than epsilon then it will explore by choosing random actions
        a = np.random.choice(env.a)
        return a


def q_hat(s, a, w):     # defining q_hat funtion
    q_val = np.dot(w, feature_vector(s, a))       # Calculating dot product of weights and feature vector
    return q_val


def feature_vector(s, a):                         # defining function to create a feature vector
    vector = []
    for x in range(len(s)):
        for y in range(len(s)):
            if s[x][y] == 9:                        # calculating position of a player
                vector = [1, x, y, x + a, y + a]      # creating feature vector of size 5 as my size of weight vector is 5. 
                # vector = [1, x, y, x+a, y+a]
    return np.array(vector)

sarsa(env, 0.001, 0.8,1000)

